# Machine_Learning
 
This is a homework for Machine Learning Lecture. I applied various machine learning algorithms, Perceptron, KNN, Logistic Regression, Gradient Descent, for two different dataset which are (https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package) and (https://www.kaggle.com/datasets/iamsouravbanerjee/house-rent-prediction-dataset).

In the first part, firstly, I opened the data through the pandas library, then I examined the data, I saw that some parts in the data were not measured and I discarded them with the dropna command, then I converted the properties that did not have a number value to a number ('WindDir9am', 'WindDir3pm', 'RainToday', 'WindGustDir' ) and assigned 1 and 0 to our target property, RainTomorrow. I removed the date and location column, which will not be of any use to us from now on. To obtain more accurate results in numerical operations, I converted all values to decimal numbers(float). I divided my data into X (feature) and Y (target). I divided these values into two: train and test. I increased the accuracy by doing Cross Validation. I applied the KNN algorithm to this data, drew ROC Curve and calculated AUC (0.81). Then I applied Logistic Regression, drew ROC Curve and calculated AUC (0.88). Finally, I applied the Perceptron algorithm and the result was not very satisfactory. As a result, it would be correct to say that Logistic Regression is the best option by looking at both the AUC and F1 score and Recall values in this data (KNN achieved a better score only in Precision value). Logistic regression works very well because the response variable is binary but the explanatory variables are continuous.

In the second part, I applied a Gradient Descent algorithm, and I realize when we change to learning rate, the iteration steps we need to done is change, for instance when learning rate=0.01 we need 75 iteration step for convergence, when learning rate=0.1, we need 61 iteration step for convergence, when learning rate=1, we need 52 iteration step for convergence. The Gradient Descent algorithm is approaching the local minimum point with iterations, I showed this in the last graphs. As the learning rate changes, the convergence image also changes.
